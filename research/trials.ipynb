{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37252b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240bd08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Documents\\\\genrative_AI_projects\\\\Source-Code-Analysis\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8adae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d948c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/OmAryan04/Medical-chatbot\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed4ed806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.document_loaders.generic import GenericLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329246f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all the source code from test_repo\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cb0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ceac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nimport vertexai\\nfrom langchain_google_vertexai import VertexAI \\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nproject_id = os.getenv(\"project_id\")\\nproject_region = os.getenv(\"region\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n# authenticate with vertexai\\nvertexai.init(project=project_id, location=project_region)\\n# load the model\\nllm = VertexAI(model_name=\"gemini-2.5-pro\")\\n\\nprompt_strict = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt_strict),\\n    (\"human\", \"{input}\"),\\n])\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt_strict)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name=\"medical-chatbot\",\\n    version=\"0.0.0\",\\n    author=\"Om Aryan\",\\n    author_email=\"omaryan392@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[]\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n\\n# now load the data, creating chunks and embedding\\nextracted_data = load_pdf_file(data=\"Data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\n\\nif index_name not in pc.list_indexes():\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384,\\n        metric=\"cosine\",\\n        spec=ServerlessSpec(\\n            cloud=\"aws\",\\n            region=\"us-east-1\"\\n        )\\n    )\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings, \\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n# create a basic logging configuration  asctime=current_time\\nlogging.basicConfig(level=logging.INFO, format=\"[%(asctime)s]: %(message)s:\")\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n        logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here are the functions which i creat for reusability\\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n#Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n# download the embedding from hugging face\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Stricter system prompt for unknown answers\\nsystem_prompt_strict = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say: \\'I don\\'t know.\\' \"\\n    \"Use three sentences maximum and keep the answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f7f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8549bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nimport vertexai\\nfrom langchain_google_vertexai import VertexAI \\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nproject_id = os.getenv(\"project_id\")\\nproject_region = os.getenv(\"region\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n# authenticate with vertexai\\nvertexai.init(project=project_id, location=project_region)\\n# load the model\\nllm = VertexAI(model_name=\"gemini-2.5-pro\")\\n\\nprompt_strict = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt_strict),\\n    (\"human\", \"{input}\"),\\n])\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt_strict)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a37925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe24fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON, # it will automatically use the context-aware splitting and tag the chunks with respect to fundtions\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811ad2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04064e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nimport vertexai\\nfrom langchain_google_vertexai import VertexAI \\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nproject_id = os.getenv(\"project_id\")\\nproject_region = os.getenv(\"region\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# authenticate with vertexai\\nvertexai.init(project=project_id, location=project_region)\\n# load the model\\nllm = VertexAI(model_name=\"gemini-2.5-pro\")\\n\\nprompt_strict = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt_strict),\\n    (\"human\", \"{input}\"),\\n])\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt_strict)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name=\"medical-chatbot\",\\n    version=\"0.0.0\",\\n    author=\"Om Aryan\",\\n    author_email=\"omaryan392@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[]\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# now load the data, creating chunks and embedding\\nextracted_data = load_pdf_file(data=\"Data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\n\\nif index_name not in pc.list_indexes():\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384,\\n        metric=\"cosine\",\\n        spec=ServerlessSpec(\\n            cloud=\"aws\",\\n            region=\"us-east-1\"\\n        )\\n    )'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings, \\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n# create a basic logging configuration  asctime=current_time\\nlogging.basicConfig(level=logging.INFO, format=\"[%(asctime)s]: %(message)s:\")\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n        logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here are the functions which i creat for reusability\\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n#Extract Data From the PDF File'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n# download the embedding from hugging face'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Stricter system prompt for unknown answers\\nsystem_prompt_strict = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say: \\'I don\\'t know.\\' \"\\n    \"Use three sentences maximum and keep the answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93337d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf5be55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8392\\3409896792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\HP\\anaconda3\\envs\\llmbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a6c26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e779704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "# checking if embedding is working or not\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c882592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95751608",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8392\\4050888053.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist() \n",
    "# i am using the database in the same session in which i created it that's why\n",
    "#  i don't need to load the database with code \n",
    "#vectordb = Chroma(persist_directory=persist_directory,\n",
    "#                  embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "530a2e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b7e9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_id = os.getenv(\"project_id\")\n",
    "project_region = os.getenv(\"region\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a1b0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "# authenticate with vertexai\n",
    "vertexai.init(project=project_id, location=project_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2be498f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "from langchain_google_vertexai import VertexAI \n",
    "llm = VertexAI(model_name=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f248132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d26aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8392\\1308165443.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# it will remember the chat history\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bcd3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efdd0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "036f3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the `download_hugging_face_embeddings` function is a Python function that initializes `HuggingFaceEmbeddings` using the `sentence-transformers/all-MiniLM-L6-v2` model and then returns the embeddings object.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00f2b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the `load_pdf_file` function is a Python function that loads all PDF files from a specified directory. It uses the `DirectoryLoader` to search for files with the `.pdf` extension and the `PyPDFLoader` class to load the content of those files. The function then returns the loaded documents.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is load_pdf_file funtion?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caf8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
